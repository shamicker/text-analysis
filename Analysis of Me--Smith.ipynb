{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Me--Smith - by Caroline Lockhart\n",
    "## My analysis of the most popular download from Project Gutenberg of July 13, 2019.\n",
    "I got the txt file from https://www.gutenberg.org/ebooks/27438"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'Me-Smith-pg27438.txt'\n",
    "\n",
    "with open(txt, 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through trial and error, I decided on the following length to omit from the analysis. The last \"ME--SMITH\" below is the title of the book before the chapter numeral starts off the book. I only want to analyse the actual text, as near as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406554 \n",
      " ï»¿The Project Gutenberg EBook of 'Me-Smith', by Caroline Lockhart\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.org\n",
      "\n",
      "\n",
      "Title: 'Me-Smith'\n",
      "\n",
      "Author: Caroline Lockhart\n",
      "\n",
      "Illustrator: Gayle Hoskins\n",
      "\n",
      "Release Date: December 8, 2008 [EBook #27438]\n",
      "\n",
      "Language: English\n",
      "\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK 'ME-SMITH' ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Produced by Roger Frank and the Online Distributed\n",
      "Proofreading Team at http://www.pgdp.net\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Illustration: \"THAT LOOK IN YOUR EYES--THAT LOOK AS IF YOU HADN'T\n",
      "NOTHIN' TO HIDE--IS IT TRUE?\" Page 59]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"ME-SMITH\"\n",
      "\n",
      "BY\n",
      "\n",
      "CAROLINE LOCKHART\n",
      "\n",
      "WITH ILLUSTRATIONS BY\n",
      "\n",
      "GAYLE HOSKINS\n",
      "\n",
      "NEW YORK\n",
      "\n",
      "GROSSET & DUNLAP\n",
      "\n",
      "PUBLISHERS\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Copyright 1911\n",
      "By J. B. Lippincott Company\n",
      "\n",
      "Published February 15, 1911\n",
      "Second printing, February 25, 1911\n",
      "Third printing, March 5, 1911\n",
      "Fourth printing, March 20, 1911\n",
      "Fifth Printing, June 5, 1911\n",
      "Sixth Printing, July 1, 1911\n",
      "Seventh Printing, August 17, 1911\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "CHAPTER                                            PAGE\n",
      "     I.  \"Me--Smith\"                                 11\n",
      "    II.  On the Alkali Hill                          18\n",
      "   III.  The Empty Chair                             29\n",
      "    IV.  A Swap in Saddle Blankets                   48\n",
      "     V.  Smith Makes Medicine with the Schoolmarm    58\n",
      "    VI.  The Great Secret                            79\n",
      "   VII.  Cupid \"Wings\" a Deputy Sheriff              95\n",
      "  VIII.  The Bug-hunter Elucidates                  110\n",
      "    IX.  Speaking Of Grasshoppers----               123\n",
      "     X.  Mother Love and Savage Passion Conflict    130\n",
      "    XI.  The Best Horse                             142\n",
      "   XII.  Smith Gets \"Hunks\"                         156\n",
      "  XIII.  Susie's Indian Blood                       162\n",
      "   XIV.  The Slayer of Mastodons                    169\n",
      "    XV.  Where a Man Gets a Thirst                  190\n",
      "   XVI.  Tinhorn Frank Smells Money                 205\n",
      "  XVII.  Susie Humbles Herself to Smith             213\n",
      " XVIII.  A Bad \"Hombre\"                             228\n",
      "   XIX.  When The Clouds Played Wolf                240\n",
      "    XX.  The Love Medicine of the Sioux             248\n",
      "   XXI.  The Murderer of White Antelope             272\n",
      "  XXII.  A Mongolian Cupid                          293\n",
      " XXIII.  In Their Own Way                           303\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIST OF ILLUSTRATIONS\n",
      "\n",
      "                                                         PAGE\n",
      "\n",
      "\"That Look in Your Eyes--That Look as if You\n",
      "Hadn't Nothin' to Hide--is it True?\"             Frontispiece\n",
      "\n",
      "\"She's a Game Kid, All Right,\" Said Smith\n",
      "to Himself at the Top of the Hill.                         22\n",
      "\n",
      "It Meant Death--but it was Wet!--it was Water!            196\n",
      "\n",
      "Smith Reached for the Trailing Rope and They\n",
      "Were Gone!                                                284\n",
      "\n",
      "They Quirted Their Horses at Breakneck Speed\n",
      "In the Direction of the Bad Lands.                        308\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"ME--SMITH\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(text), '\\n', text[:3060])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I'll put the pre-text into its own variable, and I'll get rid of the post-text too. Also by trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pretext = text[:3060]\n",
    "posttext = text[-30200:]\n",
    "# print(posttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "\n",
      "\"ME--SMITH\"\n",
      "\n",
      "\n",
      "A man on a tired gray horse reine\n",
      "**********\n",
      "uns--tell the Schoolmarm I died game,\n",
      "me--Smith!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = text[3061:-30200]\n",
    "print(data[:50])\n",
    "print('**********')\n",
    "print(data[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out some of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before being cleaned up, there are about:\n",
      "\n",
      "373293 characters \n",
      "66296 words \n",
      "12812 unique words\n",
      "\n",
      "8437 lines of text \n",
      "average of 7.858 words per line\n"
     ]
    }
   ],
   "source": [
    "print(\"Before being cleaned up, there are about:\")\n",
    "print(\"\\n{} characters \\n{} words \\n{} unique words\".format(len(data), len(data.split()), len({word:None for word in data.split()})))\n",
    "\n",
    "lines = data.split('\\n')\n",
    "print(\"\\n{} lines of text \\naverage of {:.4} words per line\".format(len(lines), np.average([len(line.split()) for line in lines])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up the data\n",
    "I want to clean up the data a bit first:\n",
    "- convert everything to lower case\n",
    "- remove the \"images\" (which are between square brackets `[]`)\n",
    "- get rid of stopwords\n",
    "- check to see if I should stem the words too\n",
    "\n",
    "I know all this can be done pretty easily using the `nltk` tools and regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may as well create a function to do this\n",
    "def text_cleaner(text):\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "    stemmed = [stemmer.stem(w) for w in words]\n",
    "    \n",
    "    return words, stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, stemmed = text_cleaner(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning it up, there are \n",
      "\n",
      "32966 words and 32966 stemmed words, and\n",
      "7077 unique words and 5066 unique stemmed words\n"
     ]
    }
   ],
   "source": [
    "print(\"After cleaning it up, there are \\n\\n{} words and {} stemmed words, and\".format(len(words), len(stemmed)))\n",
    "print(\"{} unique words and {} unique stemmed words\".format(len(set(words)), len(set(stemmed))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see what kind of common words are in there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqDistWords = nltk.FreqDist(words)\n",
    "freqDistStems = nltk.FreqDist(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 7077 samples and 32966 outcomes>\n",
      "<FreqDist with 5066 samples and 32966 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print(freqDistWords)\n",
    "print(freqDistStems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('smith', 610),\n",
       " ('susie', 295),\n",
       " ('like', 289),\n",
       " ('ralston', 228),\n",
       " ('said', 211),\n",
       " ('would', 197),\n",
       " ('eyes', 172),\n",
       " ('tubbs', 170),\n",
       " ('could', 168),\n",
       " ('upon', 166),\n",
       " ('woman', 164),\n",
       " ('one', 161),\n",
       " ('man', 151),\n",
       " ('dora', 145),\n",
       " ('mcarthur', 139),\n",
       " ('time', 135),\n",
       " ('horse', 132),\n",
       " ('little', 124),\n",
       " ('looked', 123),\n",
       " ('back', 112),\n",
       " ('white', 109),\n",
       " ('face', 108),\n",
       " ('indian', 103),\n",
       " ('get', 97),\n",
       " ('see', 96),\n",
       " ('hand', 94),\n",
       " ('never', 93),\n",
       " ('horses', 91),\n",
       " ('go', 88),\n",
       " ('come', 86),\n",
       " ('know', 86),\n",
       " ('good', 84),\n",
       " ('make', 84),\n",
       " ('say', 84),\n",
       " ('head', 83),\n",
       " ('got', 83),\n",
       " ('look', 82),\n",
       " ('saddle', 81),\n",
       " ('thought', 79),\n",
       " ('made', 78),\n",
       " ('think', 78),\n",
       " ('knew', 77),\n",
       " ('take', 77),\n",
       " ('house', 75),\n",
       " ('way', 75),\n",
       " ('schoolmarm', 74),\n",
       " ('mother', 73),\n",
       " ('something', 73),\n",
       " ('babe', 73),\n",
       " ('long', 72)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %pprint\n",
    "freqDistWords.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('smith', 610),\n",
       " ('like', 324),\n",
       " ('susi', 295),\n",
       " ('look', 252),\n",
       " ('ralston', 228),\n",
       " ('hors', 223),\n",
       " ('eye', 213),\n",
       " ('said', 211),\n",
       " ('would', 197),\n",
       " ('tubb', 170),\n",
       " ('could', 168),\n",
       " ('one', 166),\n",
       " ('upon', 166),\n",
       " ('woman', 164),\n",
       " ('time', 155),\n",
       " ('hand', 155),\n",
       " ('man', 151),\n",
       " ('indian', 150),\n",
       " ('dora', 145),\n",
       " ('mcarthur', 139),\n",
       " ('littl', 124),\n",
       " ('get', 123),\n",
       " ('back', 119),\n",
       " ('come', 119),\n",
       " ('know', 117),\n",
       " ('face', 116),\n",
       " ('go', 116),\n",
       " ('white', 112),\n",
       " ('see', 110),\n",
       " ('make', 109),\n",
       " ('say', 107),\n",
       " ('thought', 98),\n",
       " ('head', 96),\n",
       " ('never', 93),\n",
       " ('think', 92),\n",
       " ('take', 92),\n",
       " ('want', 91),\n",
       " ('saddl', 89),\n",
       " ('seem', 87),\n",
       " ('good', 85),\n",
       " ('got', 83),\n",
       " ('day', 79),\n",
       " ('made', 78),\n",
       " ('knew', 77),\n",
       " ('long', 76),\n",
       " ('hous', 76),\n",
       " ('way', 76),\n",
       " ('schoolmarm', 75),\n",
       " ('mother', 73),\n",
       " ('feel', 73)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqDistStems.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
